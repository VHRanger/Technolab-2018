{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "import sys\n",
    "### Gensim is outside the anaconda distribution ###\n",
    "### uncomment to install Gensim ###\n",
    "#!{sys.executable} -m pip install gensim\n",
    "import gensim\n",
    "import gensim.downloader as model_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained word embeddings\n",
    "# This will download 60mb of data the first time it's loaded\n",
    "word_vectors = model_api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An interactive introduction to word embeddings\n",
    "\n",
    "**Goals:**\n",
    "\n",
    "- Demystify text-based AI models\n",
    "\n",
    "\n",
    "- Convince you that this is very cool!\n",
    "\n",
    "**applications:**\n",
    "\n",
    "- Translation (eg. Google Translate)\n",
    "\n",
    "\n",
    "- Text recommendation (autocomplete)\n",
    "\n",
    "\n",
    "- Chatbots (automatic customer service)\n",
    "\n",
    "\n",
    "- Much much more!\n",
    "\n",
    "\n",
    "- [See here for state of the art on tasks](https://github.com/sebastianruder/NLP-progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a **magic trick**: \n",
    "\n",
    "\n",
    "$(Paris - France) + Russia = x$ \n",
    "\n",
    "\n",
    "which should give us like $x = Moscow$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('moscow', 0.9656671285629272),\n",
       " ('russian', 0.8811113834381104),\n",
       " ('prague', 0.8772416710853577),\n",
       " ('vienna', 0.8710137009620667),\n",
       " ('putin', 0.8702147006988525),\n",
       " ('warsaw', 0.8692157864570618),\n",
       " ('kiev', 0.8671694993972778),\n",
       " ('tokyo', 0.8649566173553467),\n",
       " ('berlin', 0.8640562295913696),\n",
       " ('47-42-17-11', 0.8622288107872009)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the most similar word to an expression\n",
    "word_vectors.most_similar_cosmul(positive=['paris', 'russia'], negative=['france'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP ADVANTAGE:** It's easy to generate datasets in NLP if you're clever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_name</th>\n",
       "      <th>title</th>\n",
       "      <th>version</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tomb of the Mask</td>\n",
       "      <td>Overall fun game...but</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ENOUGH of the ads. I know the creator wants to...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tomb of the Mask</td>\n",
       "      <td>Would be good if not for the ads</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Everything you do requires an ad, it’s really ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tomb of the Mask</td>\n",
       "      <td>AMAZING</td>\n",
       "      <td>1.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This game is so indicting and I love it</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tomb of the Mask</td>\n",
       "      <td>So many ads</td>\n",
       "      <td>1.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The game itself is really fun but you go throu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tomb of the Mask</td>\n",
       "      <td>Terrible app</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This app has a cool game in it. However the ap...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           app_name                             title version  rating  \\\n",
       "0  Tomb of the Mask            Overall fun game...but     1.6     2.0   \n",
       "1  Tomb of the Mask  Would be good if not for the ads     1.6     1.0   \n",
       "2  Tomb of the Mask                           AMAZING     1.6     5.0   \n",
       "3  Tomb of the Mask                       So many ads     1.6     4.0   \n",
       "4  Tomb of the Mask                      Terrible app     1.6     1.0   \n",
       "\n",
       "                                              review  vote_count  \n",
       "0  ENOUGH of the ads. I know the creator wants to...         0.0  \n",
       "1  Everything you do requires an ad, it’s really ...         0.0  \n",
       "2            This game is so indicting and I love it         0.0  \n",
       "3  The game itself is really fun but you go throu...         0.0  \n",
       "4  This app has a cool game in it. However the ap...         0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Generate a dataset\n",
    "### Takes apple app IDs (find them in appstore URL)\n",
    "### And gets reviews for that app\n",
    "### AppReviews is a tiny library to query AppStore JSON API\n",
    "# import AppReviews\n",
    "# df = pd.concat([\n",
    "#     AppReviews.get_reviews(1057889290),\n",
    "#     AppReviews.get_reviews(1402499966),\n",
    "#     AppReviews.get_reviews(1417799395),\n",
    "#     AppReviews.get_reviews(1403455040),\n",
    "#     AppReviews.get_reviews(585027354),\n",
    "#     AppReviews.get_reviews(454638411),\n",
    "# ])\n",
    "### Clean up NaN values ###\n",
    "# df.loc[df['review'].isna(), 'review'] = \".\"\n",
    "# df.loc[df['app_name'].isna(), \"app_name\"] = \".\"\n",
    "# df.loc[df['title'].isna(), 'title'] = \".\"\n",
    "# df.loc[df['vote_count'].isna(), 'vote_count'] = 0.0\n",
    "# df.loc[df['version'].isna(), 'version'] = -1\n",
    "# df.loc[df['rating'].isna(), 'rating'] = 2.5\n",
    "# df.to_csv('app_reviews.csv', index=False)\n",
    "\n",
    "df = pd.read_csv('app_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamental Problem:\n",
    "\n",
    "If we want to the the text to produce predictions or suggestions, we first need to translate it to a mathematical form\n",
    "\n",
    "**Naive solution:** Have each document (each review) become a list of the word it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game</th>\n",
       "      <th>gameeee</th>\n",
       "      <th>gamemodes</th>\n",
       "      <th>gameplay</th>\n",
       "      <th>gameplay10</th>\n",
       "      <th>gamer</th>\n",
       "      <th>gamers</th>\n",
       "      <th>games</th>\n",
       "      <th>gamification</th>\n",
       "      <th>gaming</th>\n",
       "      <th>...</th>\n",
       "      <th>너무</th>\n",
       "      <th>네비게이션의</th>\n",
       "      <th>사용하기</th>\n",
       "      <th>안내</th>\n",
       "      <th>좋은지도</th>\n",
       "      <th>좋음</th>\n",
       "      <th>차량</th>\n",
       "      <th>파악하여</th>\n",
       "      <th>편함</th>\n",
       "      <th>해주니</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7461 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   game  gameeee  gamemodes  gameplay  gameplay10  gamer  gamers  games  \\\n",
       "0     2        0          0         0           0      0       0      0   \n",
       "1     3        0          0         0           0      0       0      0   \n",
       "2     1        0          0         0           0      0       0      0   \n",
       "3     2        0          0         0           0      0       0      0   \n",
       "4     2        0          0         0           0      0       0      0   \n",
       "\n",
       "   gamification  gaming ...   너무  네비게이션의  사용하기  안내  좋은지도  좋음  차량  파악하여  편함  \\\n",
       "0             0       0 ...    0       0     0   0     0   0   0     0   0   \n",
       "1             0       0 ...    0       0     0   0     0   0   0     0   0   \n",
       "2             0       0 ...    0       0     0   0     0   0   0     0   0   \n",
       "3             0       0 ...    0       0     0   0     0   0   0     0   0   \n",
       "4             0       0 ...    0       0     0   0     0   0   0     0   0   \n",
       "\n",
       "   해주니  \n",
       "0    0  \n",
       "1    0  \n",
       "2    0  \n",
       "3    0  \n",
       "4    0  \n",
       "\n",
       "[5 rows x 7461 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(df['review'])\n",
    "wordLabels = vectorizer.get_feature_names()\n",
    "\n",
    "# Print example of the bag-of-words matrix\n",
    "pd.DataFrame(data=X.toarray(), columns=wordLabels).loc[:, 'game':].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to try to **predict a review's rating** with this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS R^2:  0.36698404366771886\n"
     ]
    }
   ],
   "source": [
    "# Use PCA to compress X matrix (for speedup)\n",
    "# Information on it here: https://jeremykun.com/2012/06/28/principal-component-analysis/\n",
    "# Reference here: https://onlinecourses.science.psu.edu/stat505/node/51/\n",
    "# Note model won't work without compression because so many words have only 1 entry\n",
    "COMPRESSED_SIZE = 200\n",
    "\n",
    "Xd = X.toarray()\n",
    "Xd = PCA(COMPRESSED_SIZE).fit(Xd.T).components_.T\n",
    "Xd = sm.add_constant(Xd)\n",
    "\n",
    "# OLS computed by hand for convenience\n",
    "# Ypred = X(X'X)^-1 X'Y  ----- Reference here:\n",
    "# https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf\n",
    "pred = Xd @ sc.linalg.inv(Xd.T @ Xd) @ Xd.T @ df['rating'].values\n",
    "print(\"OLS R^2: \", r2_score(df['rating'], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the problems with this approach?\n",
    "\n",
    "- Doesn't associate similar/same words\n",
    "\n",
    "\n",
    "- No information about words themselves\n",
    "\n",
    "\n",
    "- No word importance information\n",
    "\n",
    "Some fixes for bag-of-words approach are detailed in the first week of [this free NLP course](https://www.coursera.org/learn/language-processing/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A better approach to NLP\n",
    "\n",
    "Is to get a [mathematical representation of each word](https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/), and combine those into sentences.\n",
    "\n",
    "The most popular way to do this is currently to exploit words appearing in sentences together.\n",
    "\n",
    "First, let's get embeddings from the [co-occurence matrix](https://web.stanford.edu/class/cs224n/reports/2758144.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[8, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 2, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 1, 1, 1],\n",
       "        [0, 0, 0, ..., 1, 1, 1],\n",
       "        [0, 0, 0, ..., 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want a symmetric matrix where each row and column is a word\n",
    "# And each entry is how often the words appear together in a sentence\n",
    "# Matrix multiplying the bag-of-words matrix with its transpose this matrix\n",
    "Xc = (X.T @ X).todense()\n",
    "# Xc.setdiag(0) # if you want same word cooccurence to 0\n",
    "Xc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_embeddings = PCA(COMPRESSED_SIZE).fit(Xc).components_.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to find the \"closest\" word to an other word. The normal method to do this is called \"nearest neighbors\".\n",
    "\n",
    "Good article on it [here](https://www.quantamagazine.org/universal-method-to-sort-complex-information-found-20180813/). Simple method is [K-D tree](https://en.wikipedia.org/wiki/K-d_tree) but there are [more sophisticated libraries](https://github.com/spotify/annoy).\n",
    "\n",
    "The metric we use to judge how close two words are is the **cosine distance** between their vectors. This is basically the angle between the vectors, so the metric judges the difference in \"where they point to\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance on normalized vectors is cosine distance\n",
    "my_embeddings = sklearn.preprocessing.normalize(my_embeddings)\n",
    "# KD-tree uses euclidean distance\n",
    "tree = sklearn.neighbors.KDTree(my_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game :   0.0\n",
      "awsome :   0.8607745824279769\n",
      "jimmy :   0.945669374465239\n",
      "interuppting :   1.0764969308474335\n",
      "bhen :   1.0796239962633714\n"
     ]
    }
   ],
   "source": [
    "evalWord = 'game'\n",
    "k = 5\n",
    "\n",
    "dist, ind = tree.query([my_embeddings[wordLabels.index(evalWord)]], k=k)\n",
    "\n",
    "for i in range(k):\n",
    "    print(wordLabels[ind[0][i]], \":  \", dist[0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we understand the magic trick used in the beginning!**\n",
    "\n",
    "There are many better methods to generate embeddings. The most popular is [word2vec](https://www.tensorflow.org/tutorials/representation/word2vec) and [GloVE](https://nlp.stanford.edu/projects/glove/). There are also methods based on [matrix factorization](https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/) like we did. Modern techniques use recurrent neural net models [predicting words](https://thegradient.pub/nlp-imagenet/) to generate better embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
